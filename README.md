# MLfinal
	In train
	normal		875363
	----------------------------
	apach2		0
	back 		1971
	mailbomb 	0
	processtable 	0
	snmpgetattack 	0
	teardrop 	881
	smurf 		2527107
	land 		19
	neptune 	964959
	pod 		244
	udpstorm 	0
	-----------------------------
	ps 		11272
	buffer_overflow 25
	perl 		1
	rootkit 	8
	loadmodule 	9	
	xterm 		0
	sqlattack 	0
	httptunnel 	0
	----------------------------
	ftp_write 	8
	guess_passwd 	47
	snmpguess 	0
	imap 		980
	spy 		1
	warezclient 	914
	warezmaster 	18
	multihop 	7
	phf 		4
	imap 		980
	named 		0
	sendmail 	0
	xlock 		0
	xsnoop 		0
	worm 		0
	----------------------------
	nmap 		2080
	ipsweep 	11272
	portsweep 	9328
	satan 		14309
	mscan 		0
	saint 		0
	worm 		0
### yoshie
#### after deleting same data:	
	class0: 734536
	class1: 230990
	class2: 43
	class3: 902
	class4: 12757
#### up to now, 0.95 kaggle performance with 1 NN layer, and 1 softmax layer. 

### changhc
####cyber.py
	ver 1:
	score = 0.95682 using 2 layers (32, 32) (relu, relu) and 1 softmax output.

##TODO
* Deal with unbalanced training set
	* https://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set
	* http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/
